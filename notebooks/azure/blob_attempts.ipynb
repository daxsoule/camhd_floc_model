{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamHD Video File to Azure Blob with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload file from URL to Microsoft Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService, PublicAccess\n",
    "from azure.storage.blob.models import Blob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# load Azure storage account credentials\n",
    "with open('/home/jovyan/.azure_credentials_ooitest.yaml', 'r') as f:\n",
    "    credentials = yaml.load(f)\n",
    "azure_storage_account_name = credentials['azure_storage_account_name']\n",
    "azure_storage_account_key = credentials['azure_storage_account_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_blob_service = BlockBlobService(account_name='azure_storage_account_name', account_key='azure_storage_account_key')\n",
    "container_name ='camhdtester'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_sample():\n",
    "       block_blob_service.copy_blob(container_name,'rawdata.oceanobservatories.org','https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/06/CAMHDA301-20190706T060000Z.log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main method.\n",
    "if __name__ == '__main__':\n",
    "    run_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Server to Local Storage with delayed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a recent mp4 for a smaller size from ooi's raw data server\n",
    "import pandas as pd\n",
    "## List of Inputs for Dask\n",
    "urls = [\"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T050000.mp4\",\n",
    "        \"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T051500.mp4\", \n",
    "        \"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4\"]\n",
    "link = 'https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4'\n",
    "paths = '/home/jovyan/floc_data/'\n",
    "local_filename = []\n",
    "for i in urls:\n",
    "    local_filename.append(i.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying dict comprehensions, because dataframes and lists are writing all downloads to the same file path... writing over the same file.\n",
    "download_dict = dict(zip(urls, local_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to check your work\n",
    "# print(download_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get yer dask going\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Dl Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will try to DL with Dask\n",
    "### the print command did not work but I would like it to!\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "@delayed\n",
    "def delayed_dl_file(download_dict):\n",
    "    ## we use k and v here for key and value from our dictionary (download_dict)\n",
    "    for k,v in download_dict.items():\n",
    "        # NOTE the stream=True parameter\n",
    "        r = requests.get(k, stream=True)\n",
    "        with open(os.path.join(paths, v), 'wb') as f:\n",
    "            #Use chunks to download the file in \"chunks\" instead of loading the entire file at once.\n",
    "            for chunk in r.iter_content(chunk_size=5024): \n",
    "               ## 200 is the code for HTTP status = \"OK\"\n",
    "                if r.status_code == 200:\n",
    "                    f.write(chunk)\n",
    "        print(\"File\" , k , \"created in\", paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up your function to compute\n",
    "delayed_dl = [] \n",
    "delayed_dl = delayed_dl_file(download_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(delayed_dl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for multiple file downloads from the server without Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a recent mp4 for a smaller size from ooi's raw data server\n",
    "import pandas as pd\n",
    "## List to dl\n",
    "urls = [\"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T050000.mp4\", \"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T051500.mp4\", \"https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4\"]\n",
    "link = 'https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4'\n",
    "paths = '/home/jovyan/floc_data/'\n",
    "local_filename = []\n",
    "for i in urls:\n",
    "    local_filename.append(i.split('/')[-1])\n",
    "#Check your filename ;)\n",
    "#print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying dict comprehensions, because dataframes and lists are writing all downloads to the same file path... writing over the same file.\n",
    "download_dict = dict(zip(urls, local_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(download_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create our download function without Dask\n",
    "# Stream is used to download media files with requests. Use chunks to download the file in \"chunks\" instead of loading the entire file at once.\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def dl_file(download_dict):\n",
    "    for k,v in download_dict.items():\n",
    "        r = requests.get(k, stream=True)\n",
    "        with open(os.path.join(paths, v), 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=5024): \n",
    "                ## 200 is the code for HTTP status = \"OK\"\n",
    "                if r.status_code == 200:\n",
    "                    f.write(chunk)\n",
    "    print(\"File\" , filename , \"created in\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_file(download_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I wanted to do this with a dataframe and not a dictionary, but I failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a dataframe to hold our information\n",
    "# downloads = pd.DataFrame({'url': urls, 'filename': local_filename, 'path':paths}, index=[1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that the files and urls match up\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# print(downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Single File Raw Server to Local Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_video(video):\n",
    "#     try:\n",
    "#         return r = requests.get(video, stream=True)\n",
    "#         with open(os.path.join(path, filename), 'wb') as f:\n",
    "#             for chunk in r.iter_content(chunk_size=5024): \n",
    "#                 if chunk: \n",
    "#                     f.write(chunk)\n",
    "#     except:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It works! Now, if you wanna do some directory work look below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell defines the function that will save our drive anon file in our data directory\n",
    "# def run(): \n",
    "#     owd = os.getcwd()\n",
    "#     #first change dir to path\n",
    "#     os.chdir(path)\n",
    "#     print(\"Now working in\", os.getcwd(),)\n",
    "#     #run download function to save video to data directory\n",
    "#     download_file(url)\n",
    "#     print(\"File\" , filename , \"created in\", path)\n",
    "#     #change dir back to original working directory (owd)\n",
    "#     os.chdir(owd)\n",
    "#     print(\"Now working back in\", os.getcwd(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we create our directory for floc data\n",
    "# # ->>>>>>>> move this to a relative directory. \n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# for url in urls:\n",
    "#     filename = url.split('/')[-1]   \n",
    "#     if not os.path.exists(path+filename):\n",
    "#         os.mkdir(path+filename)\n",
    "#         print(\"Path\", path+filename, \"now paved\")\n",
    "#     else:    \n",
    "#         print(\"Directory\" , path+filename , \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob It!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# load Azure storage account credentials\n",
    "with open('/home/jovyan/.azure_credentials_ooitest.yaml', 'r') as f:\n",
    "    credentials = yaml.load(f)\n",
    "azure_storage_account_name = credentials['azure_storage_account_name']\n",
    "azure_storage_account_key = credentials['azure_storage_account_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.storage.blob  as ASB\n",
    "import azure.storage.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service = ASB.BlockBlobService(azure_storage_account_name, account_key=azure_storage_account_key)\n",
    "\n",
    "#content_settings = ContentSettings(content_type = \"video/mov\")\n",
    "#blob_service.create_blob_from_path(\"mycontainer\",\"myblockblob\",\"sunset.png\",content_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/jovyan/vent_two.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = 'blobtest2'\n",
    "container_name = 'movtest'\n",
    "#blob_service.create_container(container_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service.create_blob_from_path(container_name, blob_name, source,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all blobs in container\n",
    "blob = blob_service.list_blobs(container_name)\n",
    "#for a in blob:\n",
    "       #blob_service.delete_blob(container_name = container_name, blob_name = a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(list(blob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_blob = list(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_blob.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_block_from_url(container_name, blob_name, copy_source_url, timeout=20):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a container for the velocity data\n",
    "#from azure.storage.blob import BlockBlobService\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)\n",
    "container_name = 'movtest'\n",
    "blob_service.create_container(container_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure.append_blob_from_path(container_name, blob_name, file_path, validate_content=False, maxsize_condition=None, progress_callback=None, lease_id=None, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service.create_file_from_bytes(share_name, directory_name, container_name, file, index=0, count=None, content_settings=None, metadata=None, validate_content=False, progress_callback=None, max_connections=2, timeout=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
