{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamHD Video File to Azure Blob with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by downloading an MOV from the raw data server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a recent mp4 for a smaller size from ooi's raw data server\n",
    "import pandas \n",
    "link = 'https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4'\n",
    "# filename = url.split('/')[-1]\n",
    "path = '/home/jovyan/floc_data/'\n",
    "\n",
    "#Check your filename ;)\n",
    "#print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of Inputs for Dask\n",
    "urls = ['https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T050000.mp4', 'https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T051500.mp4', 'https://rawdata.oceanobservatories.org/files/RS03ASHS/PN03B/06-CAMHDA301/2019/07/01/CAMHDA301-20190701T043000.mp4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Server to Local Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we create our download function without Dask\n",
    "# # Stream is used to download media files with requests. Use chunks to download the file in \"chunks\" instead of loading the entire file at once.\n",
    "# import requests\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# def download_file(url):\n",
    "#     local_filename = url.split('/')[-1]\n",
    "#     # NOTE the stream=True parameter\n",
    "#     r = requests.get(url, stream=True)\n",
    "#     with open(os.path.join(path, local_filename), 'wb') as f:\n",
    "#         ##for chunk in progress.bar(r.iter_content(chunk_size=5024), expected_size=(total_length/5024) + 1): \n",
    "#         for chunk in r.iter_content(chunk_size=5024): \n",
    "#             if chunk: \n",
    "#                 f.write(chunk)\n",
    "#     print(\"File\" , filename , \"created in\", path)\n",
    "\n",
    "# #download_file(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It works! Now let us try something that requires more thinking\n",
    "### Raw Server to Local Storage with delayed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get yer dask going\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(threads_per_worker=4, n_workers=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create our download function for Dask\n",
    "# Stream is used to download media files with requests. Use chunks to download the file in \"chunks\" instead of loading the entire file at once.\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "@delayed\n",
    "def delayed_get_file(url):\n",
    "    # NOTE the stream=True parameter\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(os.path.join(path, filename), 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=5024): \n",
    "            if chunk: \n",
    "                f.write(chunk)\n",
    "    print(\"File\" , filename , \"created in\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will generate a set of inputs on which we want to run our simulation function.\n",
    "distributed_download = []\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]\n",
    "    data = delayed_get_file(url)\n",
    "    distributed_download.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check here: https://stackoverflow.com/questions/54412569/best-way-to-download-process-and-concat-into-tfrecords-using-dask\n",
    "https://examples.dask.org/applications/embarrassingly-parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Way Below Did not Work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 2nd cell was made to fix the error below, the 2nd cell works, but the error persists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/conda/bin/dask-worker in delayed_get_file()\n",
    "#       9     # NOTE the stream=True parameter\n",
    "#      10     r = requests.get(url, stream=True)\n",
    "# ---> 11     with open(os.path.join(path, filename), 'wb') as f:\n",
    "#      12         ##for chunk in progress.bar(r.iter_content(chunk_size=5024), expected_size=(total_length/5024) + 1):\n",
    "#      13         for chunk in r.iter_content(chunk_size=5024):\n",
    "\n",
    "# FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/floc_data/CAMHDA301-20190701T043000.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create our directory for floc data\n",
    "# ->>>>>>>> move this to a relative directory. \n",
    "import os\n",
    "import sys\n",
    "\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]   \n",
    "    if not os.path.exists(path+filename):\n",
    "        os.mkdir(path+filename)\n",
    "        print(\"Path\", path+filename, \"now paved\")\n",
    "    else:    \n",
    "        print(\"Directory\" , path+filename , \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will try to DL with Dask\n",
    "from dask import delayed, compute\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "@delayed\n",
    "def delayed_get_file(url):\n",
    "    # NOTE the stream=True parameter\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(os.path.join(path, filename), 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=5024): \n",
    "            if chunk: \n",
    "                f.write(chunk)\n",
    "    print(\"File\" , filename , \"created in\", path)\n",
    "\n",
    "# @delayed\n",
    "# def delayed_prores_to_blob(blob_service, container_name, blob_name, frame_data):\n",
    "#     blob_service.create_blob_from_bytes(container_name, blob_name, frame_data)\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_download = []\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]\n",
    "    data = delayed_get_file(url)\n",
    "    delayed_download.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_download[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ehh = compute(*delayed_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this to run the run function outlined above\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell defines the function that will save our drive anon file in our data directory\n",
    "# def run(): \n",
    "#     owd = os.getcwd()\n",
    "#     #first change dir to path\n",
    "#     os.chdir(path)\n",
    "#     print(\"Now working in\", os.getcwd(),)\n",
    "#     #run download function to save video to data directory\n",
    "#     download_file(url)\n",
    "#     print(\"File\" , filename , \"created in\", path)\n",
    "#     #change dir back to original working directory (owd)\n",
    "#     os.chdir(owd)\n",
    "#     print(\"Now working back in\", os.getcwd(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob It!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# load Azure storage account credentials\n",
    "with open('.azure_credentials_ooitest.yaml', 'r') as f:\n",
    "    credentials = yaml.load(f)\n",
    "azure_storage_account_name = credentials['azure_storage_account_name']\n",
    "azure_storage_account_key = credentials['azure_storage_account_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.storage.blob  as ASB\n",
    "import azure.storage.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service = ASB.BlockBlobService(azure_storage_account_name, account_key=azure_storage_account_key)\n",
    "\n",
    "#content_settings = ContentSettings(content_type = \"video/mov\")\n",
    "#blob_service.create_blob_from_path(\"mycontainer\",\"myblockblob\",\"sunset.png\",content_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/jovyan/output_001.webm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = 'blob_test'\n",
    "container_name = 'movtest'\n",
    "blob_service.create_container(container_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service.create_blob_from_path(container_name, blob_name, source,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all blobs in container\n",
    "blob = blob_service.list_blobs(container_name)\n",
    "#for a in blobs:\n",
    " #    blob_service.delete_blob(container_name = container_name, blob_name = a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(list(blob)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_blob = list(blob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_block_from_url(container_name, blob_name, copy_source_url, timeout=20):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a container for the velocity data\n",
    "#from azure.storage.blob import BlockBlobService\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)\n",
    "container_name = 'movtest'\n",
    "blob_service.create_container(container_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure.append_blob_from_path(container_name, blob_name, file_path, validate_content=False, maxsize_condition=None, progress_callback=None, lease_id=None, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service.create_file_from_bytes(share_name, directory_name, container_name, file, index=0, count=None, content_settings=None, metadata=None, validate_content=False, progress_callback=None, max_connections=2, timeout=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
